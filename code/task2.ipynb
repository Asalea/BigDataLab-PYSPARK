{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove exising file: ./../result/task2.out\n",
      "Processing: ./../dataset/name/entire.tmp\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def GetFileName(file_dir, suffix):\n",
    "    fileNames =[]\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == suffix:\n",
    "                fileNames.append(os.path.join(root, file))\n",
    "    for i in range(0, len(fileNames)):\n",
    "        fileNames[i] = fileNames[i].replace('\\\\', '/')\n",
    "    return fileNames\n",
    "\n",
    "def GetEntireFile(filenames):\n",
    "    filenames = GetFileName('./../dataset/name', '.name')\n",
    "    outPath = './../dataset/name/entire.tmp'\n",
    "    if not os.path.exists(outPath):\n",
    "        print('Entire file not exists.Now creating...')\n",
    "        fout = open(outPath, 'a', encoding='utf-8')\n",
    "        for filename in filenames:\n",
    "            fin = open(filename, 'r', encoding='utf-8')\n",
    "            doc = fin.read()\n",
    "            fout.write(doc)\n",
    "        fout.close()\n",
    "    return outPath\n",
    "    \n",
    "def combine(L):\n",
    "    output = set()\n",
    "    for i  in range(0, len(L)):\n",
    "        for j in range(0, len(L)):\n",
    "            if L[i] != L[j]:\n",
    "                output.add((L[i], L[j]))\n",
    "    return list(output)\n",
    "\n",
    "def drive(taskName):\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(taskName)\\\n",
    "        .getOrCreate()\n",
    "    filenames = GetFileName('./../dataset/name', '.name')\n",
    "    outPath = './../result/task2.out'\n",
    "    if os.path.exists(outPath):\n",
    "        os.remove(outPath)\n",
    "        print(\"Remove exising file: \" + outPath)\n",
    "    inPath = GetEntireFile(filenames)\n",
    "    #inPath = './../dataset/name/金庸01飞狐外传.name'\n",
    "    #inPath = './../dataset/name/entire.name'\n",
    "    mapreduce(inPath, spark)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "def mapreduce(inPath,spark):\n",
    "    print('Processing: ' + inPath)\n",
    "    lines  = spark.read.text(inPath) \\\n",
    "            .rdd.map(lambda x:x[0]) \\\n",
    "            .map(lambda x: x.split('|'))\n",
    "    #print(lines.collect())\n",
    "    counts = lines.flatMap(combine).map(lambda x:(x,1)).reduceByKey(add)\n",
    "    output = counts.sortByKey(ascending=True).collect()\n",
    "    fout = open('./../result/task2.out', 'w', encoding='utf-8')\n",
    "    for (word, count) in output:\n",
    "        fout.write(word[0] + ',' + word[1] + ',' + str(count) + '\\n')\n",
    "    fout.close()\n",
    "    \n",
    "drive('Task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
