{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jieba as jb\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def createNameMap(filepath):\n",
    "    names = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as fin:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            line = line.strip('\\n')\n",
    "            names.append(line)\n",
    "            line = fin.readline()\n",
    "    name_dict = {}\n",
    "    for name in names:\n",
    "        name_dict[name] = 1\n",
    "    return name_dict\n",
    "\n",
    "def GetFileName(file_dir, suffix):\n",
    "    fileNames =[]\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == suffix:\n",
    "                fileNames.append(os.path.join(root, file))\n",
    "    for i in range(0, len(fileNames)):\n",
    "        fileNames[i] = fileNames[i].replace('\\\\', '/')\n",
    "    return fileNames\n",
    "\n",
    "def fetchName(text, name_dict):\n",
    "    name_list = []\n",
    "    for word in text:\n",
    "        if word in name_dict:\n",
    "            name_list.append(word)\n",
    "    return name_list\n",
    "        \n",
    "def drive(taskName):\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(taskName)\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    filename_list = GetFileName('./../dataset/novels', '.txt')\n",
    "    name_dict = createNameMap('./../dataset/people_name_list.txt')\n",
    "    jb.load_userdict('./../dataset/people_name_list.txt')\n",
    "    #inPath = './../dataset/novels/test.txt'\n",
    "    for inPath in filename_list:\n",
    "        print('Processing: ' + inPath)\n",
    "        mapreduce(inPath, name_dict, spark)\n",
    "    \n",
    "    print('Please type any key to stop task.')\n",
    "    input('')\n",
    "    spark.stop()\n",
    "    print('Done.')\n",
    "    \n",
    "def mapreduce(inPath, name_dict, spark):\n",
    "    lines  = spark.read.text(inPath) \\\n",
    "            .rdd.map(lambda x:x[0]) \\\n",
    "            .map(lambda s: jb.lcut(s)) #此处不知为何会出现人名分词错误的情况 如'徐铮本'\n",
    "    #print(lines.collect())\n",
    "    names = lines.map(lambda x:fetchName(x, name_dict))\n",
    "    output = names.collect()\n",
    "    outPath = './../dataset/name/' + re.match(r'./../dataset/novels/(.*).txt', inPath).group(1) + '.name'\n",
    "    fout = open(outPath, 'w', encoding='utf-8')\n",
    "    for nameList in output:\n",
    "        if(len(nameList) >= 1):\n",
    "            fout.write('|'.join(nameList) + '\\n')\n",
    "    fout.close()\n",
    "drive('task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
